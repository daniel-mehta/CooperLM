{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcfb8d00-bb6e-4c83-bff0-9d2694e30cac",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Grabs data from wikipedia, BookCorpus, and Pilesubset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45eda338-2881-4213-941d-e80d5a8293ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect, DetectorFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6453b76-4324-411a-91f7-7f2606e8fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = os.path.join(\"raw_data\", \"combined_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e7590d-fec4-482f-bbdc-b64e83fa64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia...\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"raw_data\", exist_ok=True)\n",
    "# 1. Load Wikipedia (1%)\n",
    "print(\"Loading Wikipedia...\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:1%]\", trust_remote_code=True)\n",
    "wiki = wiki.filter(lambda x: len(x['text']) > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c11517-dd02-4eda-b111-41e8c32bd413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BookCorpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|███████████████████████████████████████████████████████████| 1.18G/1.18G [01:42<00:00, 11.6MB/s]\n",
      "Generating train split: 100%|███████████████████████████████████| 74004228/74004228 [08:09<00:00, 151118.33 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████| 74004228/74004228 [01:27<00:00, 847086.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 2. Load BookCorpus\n",
    "print(\"Loading BookCorpus...\")\n",
    "books = load_dataset(\"bookcorpus\", split=\"train\", trust_remote_code=True)\n",
    "books = books.filter(lambda x: len(x['text']) > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9d7169-d167-4fb1-9e03-c52e82cc05f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenWebText...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|█████████████████████████████████████████████████████████████| 21/21 [18:33<00:00, 53.02s/files]\n",
      "Generating train split: 100%|███████████████████████████████████████| 8013769/8013769 [13:21<00:00, 9997.62 examples/s]\n",
      "Filter: 100%|█████████████████████████████████████████████████████████| 80138/80138 [00:00<00:00, 230282.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 3. Load OpenWebText\n",
    "print(\"Loading OpenWebText...\")\n",
    "pile = load_dataset(\"openwebtext\", split=\"train[:1%]\", trust_remote_code=True)\n",
    "pile = pile.filter(lambda x: len(x['text']) > 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40eef9f-441f-480c-8228-a2f6f52dd464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging datasets...\n",
      "Saving to file...\n",
      "Saved 1,150,270 entries to raw_data\\combined_text.txt\n"
     ]
    }
   ],
   "source": [
    "# 4. Merge datasets\n",
    "print(\"Merging datasets...\")\n",
    "combined = concatenate_datasets([wiki, books, pile])\n",
    "\n",
    "# 5. Save to text file\n",
    "print(\"Saving to file...\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in combined:\n",
    "        f.write(entry['text'].strip().replace(\"\\n\", \" \") + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(combined):,} entries to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9994f97-18a2-4da4-8489-be0f9e7b603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia dataset size: 64,098\n"
     ]
    }
   ],
   "source": [
    "print(f\"Wikipedia dataset size: {len(wiki):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c3175-67f9-4512-b674-4c74466b651a",
   "metadata": {},
   "source": [
    "## Start here if Dataset Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4974c100-2459-4aa9-9bae-7ed79cafc384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans raw text by removing excessive whitespace and HTML tags.\n",
    "\n",
    "    Steps:\n",
    "    - Collapses multiple spaces, tabs, and newlines into a single space\n",
    "    - Removes HTML tags using regex\n",
    "    - Strips leading/trailing whitespace\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37578946-3a17-44dc-8881-c235fbc95b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(text):\n",
    "    \"\"\"\n",
    "    Calculates Shannon entropy of a string.\n",
    "\n",
    "    Useful to measure text randomness:\n",
    "    - Natural language usually has entropy > 3.0\n",
    "    - Low-entropy suggests repetitive or spammy text\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: Shannon entropy value.\n",
    "    \"\"\"\n",
    "    prob = [freq / len(text) for freq in Counter(text).values()]\n",
    "    return -sum(p * math.log2(p) for p in prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde1fa58-5443-433a-8fba-1dce020fc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_repeated_patterns(text):\n",
    "    \"\"\"\n",
    "    Detects low-quality repetitive patterns in text.\n",
    "\n",
    "    Flags:\n",
    "    - Single-character repetition (e.g., \"aaaaaaa\")\n",
    "    - Repeated words (e.g., \"yes yes yes yes\")\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to scan.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if repetitive pattern found, else False.\n",
    "    \"\"\"\n",
    "    return bool(\n",
    "        re.search(r'(.)\\1{6,}', text) or      \n",
    "        re.search(r'\\b(\\w+)\\b(\\s+\\1\\b){3,}', text)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1825c7fe-95e3-4cf9-b4cd-0b08e121ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "def is_english(text):\n",
    "    \"\"\"\n",
    "    Determines whether the given text is written in English using language detection.\n",
    "\n",
    "    This function uses the `langdetect` library to analyze the first 1000 characters\n",
    "    of the input text. If the detected language is English (\"en\"), the function returns True.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text is detected as English, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text[:1000]) == \"en\"\n",
    "    except:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01f63247-3ae3-4589-bcd1-04416a149b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering: 41338it [02:45, 250.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 41,094 clean entries to raw_data/cleaned_combined.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = \"raw_data/combined_text.txt\"\n",
    "output_path = \"raw_data/cleaned_combined.txt\"\n",
    "\n",
    "count = 0\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as in_f:\n",
    "        for line in tqdm(in_f, desc=\"Filtering\"):\n",
    "            # Step 1: Clean text\n",
    "            cleaned = clean_text(line)\n",
    "\n",
    "            # Step 2: Language detection\n",
    "            try:\n",
    "                if detect(cleaned[:1000]) != \"en\":\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Step 3: Minimum length check\n",
    "            if len(cleaned) < 200:\n",
    "                continue\n",
    "\n",
    "            # Step 4: Shannon entropy check\n",
    "            if shannon_entropy(cleaned) <= 3.0:\n",
    "                continue\n",
    "\n",
    "            # Step 5: Pattern repetition check\n",
    "            if has_repeated_patterns(cleaned):\n",
    "                continue\n",
    "\n",
    "            # If all checks pass, save the line\n",
    "            out_f.write(cleaned + \"\\n\")\n",
    "            count += 1\n",
    "\n",
    "print(f\"Saved {count:,} clean entries to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a805d8c-b40b-405c-b874-ee923c4ee2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
