{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9d9abe-50da-4e00-bf20-3129d7480994",
   "metadata": {},
   "source": [
    "# quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e168e0b-4df9-48b3-8ee3-c7552982704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import HfApi, create_repo, login # only if uploading the model to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd466ec-b949-4453-9309-1143773d2ba0",
   "metadata": {},
   "source": [
    "## Load & Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9d94a-7832-4c60-a7a7-ed3b42a5f1f3",
   "metadata": {},
   "source": [
    "### Choose Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929eb3a3-9d5f-4967-b5d1-681465a01f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with any model you want to quantize\n",
    "# Examples:\n",
    "# - \"gpt2\" (official GPT-2 model)\n",
    "# - \"tiiuae/falcon-rw-1b\" (Falcon 1B)\n",
    "# - \"mehta/CooperLM-354M\" (My custom LLM)\n",
    "model_id = \"mehta/CooperLM-354M\"  # from Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e9ed2-25dd-4427-96a7-f18d7ae68c20",
   "metadata": {},
   "source": [
    "### Quantize with BitsAndBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2584e1c0-58de-465b-a135-21039b09a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for 4-bit quantization using NF4 + double quant\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Enable 4-bit loading\n",
    "    bnb_4bit_compute_dtype=\"float16\", # Computation precision (FP16 usually works)\n",
    "    bnb_4bit_use_double_quant=True, # Improves accuracy of quantization\n",
    "    bnb_4bit_quant_type=\"nf4\" # Type of quantization (Normal Float 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4010f6f1-90af-4a4a-850f-c18b373ba563",
   "metadata": {},
   "source": [
    "### Load and Quantize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e40239d-08f1-4bfe-a5d2-372617938e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and quantize the model using Hugging Face Transformers\n",
    "# device_map=\"auto\" will use your GPU if available\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer (not quantized - this remains in full precision)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f33e8-b308-410e-bb73-7a18b3b4f52a",
   "metadata": {},
   "source": [
    "### Save the Quantized Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "539a4168-db34-4a1e-a70e-4e93bdb78a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CooperLM-354M-quantized\\\\tokenizer_config.json',\n",
       " 'CooperLM-354M-quantized\\\\special_tokens_map.json',\n",
       " 'CooperLM-354M-quantized\\\\vocab.json',\n",
       " 'CooperLM-354M-quantized\\\\merges.txt',\n",
       " 'CooperLM-354M-quantized\\\\added_tokens.json',\n",
       " 'CooperLM-354M-quantized\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save quantized model & tokenizer to disk\n",
    "output_dir = \"CooperLM-354M-quantized\"\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ba883-7027-420b-84d1-2cd92b58f63a",
   "metadata": {},
   "source": [
    "### Upload to Hugging Face (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e64a158c-0097-415b-b2b6-7255fdfb5381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/mehta/CooperLM-354M-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='mehta/CooperLM-354M-4bit')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "create_repo(\"CooperLM-354M-4bit\", repo_type=\"model\", token=\"your_huggingface_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f97a9a74-9cc4-484f-952f-2838ad1adc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ca00a1a7048b587b69035101c40c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/260M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mehta/CooperLM-354M-4bit/commit/a254fb2b1d1bf0761bf273ab05212d5506e8d12a', commit_message='Upload folder using huggingface_hub', commit_description='', oid='a254fb2b1d1bf0761bf273ab05212d5506e8d12a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mehta/CooperLM-354M-4bit', endpoint='https://huggingface.co', repo_type='model', repo_id='mehta/CooperLM-354M-4bit'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You must have a repo created at https://huggingface.co/new\n",
    "# If not created yet, run:\n",
    "# from huggingface_hub import create_repo\n",
    "# create_repo(\"mehta/CooperLM-354M-quantized\", repo_type=\"model\")\n",
    "login(token=\"your_huggingface_token\")\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_folder(\n",
    "    folder_path=\"CooperLM-354M-quantized\",             # local quantized model files\n",
    "    repo_id=\"mehta/CooperLM-354M-4bit\",                # NEW repo (must already exist)\n",
    "    repo_type=\"model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8338f157-6a12-40ca-9743-934da212ba8f",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20c44fa0-38e4-4f4a-81e6-65c1caa727f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3216732a2053431a8aaf07637c11f669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/528 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\danie\\.cache\\huggingface\\hub\\models--mehta--CooperLM-354M-4bit. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f79a5a0d8014a9c9104098a3302aaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad90f5a9d7a4a759e1c53104c377b2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0958c48f8c624b7698a611eced33c77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a7a6e7c086497aa8cd99a8623278fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6260ffe41143c0beb41f82e239ec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3556160d35994a66bcf8fc7af66fce56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/260M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the distant future, the main. The earliest known as \"the right\" between the \"I have been the same \"the most widely found.\" It was \"the greatest single-bit-like\" and has a number of all of any other objects. It was not been used as \"the same \"on-like\" of the first described in the first year. In this time, the 20th century, the original case, the B. The term was reported that were to be accepted\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model from Hugging Face Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mehta/CooperLM-354M-4bit\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mehta/CooperLM-354M-4bit\")\n",
    "\n",
    "# Optional: Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Prompt\n",
    "prompt = \"In the distant future,\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,\n",
    "        temperature=0.8,\n",
    "        do_sample=True,\n",
    "        top_p=0.95\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17f47a-a86c-4e09-88af-799767d743b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
