{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b763ec07-2f44-4694-8a86-32f158296e66",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef010d2-19cd-4a24-8821-803c753d576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "MODEL_PATH = \"CooperLM-354M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a66af7-ce3b-4c4d-b1a2-aa4bf2de418e",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f25c38d2-ad3a-4cc8-80c5-49efe2879ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(256, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=3072, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=1024)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=4096, nx=1024)\n",
       "          (c_proj): Conv1D(nf=1024, nx=4096)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268d6d5-cc8c-45ce-b838-6f526b065702",
   "metadata": {},
   "source": [
    "## Evaluating Perplexity on Eval Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba57adb9-8a93-45f5-a61c-5d5e5babd588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|                                                                             | 0/1000 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████| 1000/1000 [00:28<00:00, 34.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity (1000 sample subset): 262.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    full_eval_dataset = load_from_disk(\"tokenized_data/cooper_subset_100k\").train_test_split(test_size=0.1)[\"test\"]\n",
    "except:\n",
    "    full_eval_dataset = None\n",
    "\n",
    "def compute_perplexity(model, dataset, tokenizer, block_size=256, sample_size=1000):\n",
    "    if dataset is None:\n",
    "        print(\"No eval dataset found.\")\n",
    "        return None\n",
    "\n",
    "    # Take a random subset of 1000 samples\n",
    "    random.seed(8232010) \n",
    "    indices = random.sample(range(len(dataset)), sample_size)\n",
    "    subset = Subset(dataset, indices)\n",
    "\n",
    "    dataloader = DataLoader(subset, batch_size=1)\n",
    "    losses = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = torch.tensor(batch['input_ids']).to(model.device)\n",
    "            outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "if full_eval_dataset:\n",
    "    perp = compute_perplexity(model, full_eval_dataset, tokenizer)\n",
    "    print(f\"\\nPerplexity (1000 sample subset): {perp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d211d0d-d19f-4739-94b2-0f2ea09bcd74",
   "metadata": {},
   "source": [
    "## Interactive Single Prompt Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad59229b-58dc-4e3b-92cf-ac6444269e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Prompt:  Hello World\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World Series in 1969 – The original song and the world's home, who did not run to a great deal with other of the band. The following the first stage to go in the club to a young team to the band, and the same to have been a young player, and the band, who did not become the most influential in a part of the United States and his house. It was \"The home\" during the film. He also been the band was played on a short-\n"
     ]
    }
   ],
   "source": [
    "def generate(prompt, max_length=100, temperature=0.9, top_p=0.95):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = input(\"Enter Prompt: \")\n",
    "print(generate(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbf6ec-6ba7-4f28-9074-bd21d58daf8b",
   "metadata": {},
   "source": [
    "##  Batch Prompt Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec7f7b65-8e20-4130-9970-a8b1d0e945d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Completion Test ===\n",
      "\n",
      "Prompt: Hello, my name is\n",
      "Hello, my name is a long to the main system, and are the world's most famous, the first-known, the city's first known as of the only to the most popular and city. The country was considered to a large use of the use of the use of the city. The population of the population of the town was originally, and more than the city's city's population of the most common buildings. Aed town has a separate city. It is a result for local cultural\n",
      "============================================================\n",
      "\n",
      "Prompt: In a distant galaxy,\n",
      "In a distant galaxy, the Sun were a high-andated-known by an iron, and the other materials were used to the use of the tail, and the other species. The last of the water was used to its the first century. At the river-based and in the early 1990s were the largest water and was discovered for the late 20th century. The modern era, the area of the population grew the 20th century. It was developed in the southern and the 17th\n",
      "============================================================\n",
      "\n",
      "Prompt: The secret to happiness is\n",
      "The secret to happiness is the next in the late as in the \"the world\" or \"It \"a\". It is very likely to a 'the second, however, it is, a very clear the 'the final as \"bian people of the first and we shall have a single sense\", and 'm\". However, the name of the word are written by an example\". History The phrase of the word is derived from the first \"a\" in any \"God\", but the \"\n",
      "============================================================\n",
      "\n",
      "Prompt: The history of Canada begins with\n",
      "The history of Canada begins with the main American American School. The first largest city has been the top of the South Africa, a small largest area of the largest city's longest-year century to the city centre of the Caribbean centre (see the most recently with some of a population. The city's largest in the city's largest-20 to the most important city of the Atlantic is the highest level, but a very than half of the population by the world's population are among a population, it\n",
      "============================================================\n",
      "\n",
      "Prompt: Artificial intelligence will change the world by\n",
      "Artificial intelligence will change the world by John C. The first two children was first popular in 1794s of the first successful by the early years. In 18th century, Babbage moved for example to a \"for other people by the time of the other other names of the late 20th century; the modern Americans, and 18th century. A \"Cau of 17th century the use of the 18th century, who was the early-term and more than all of the most\n",
      "============================================================\n",
      "\n",
      "Prompt: The country Malta is\n",
      "The country Malta is the largest world of the region. The largest cultural schools is of the largest cities of the central and its city's the world's city. The population are also in its most populous and west, and more than the world, the Central part of the largest and in the Caribbean regions, it is mainly in most than the largest municipalities, the Black Forest, but the Gulf of the state in the North Pacific. It is several of the country's largest tourist country. Some of the\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"In a distant galaxy,\",\n",
    "    \"The secret to happiness is\",\n",
    "    \"The history of Canada begins with\",\n",
    "    \"Artificial intelligence will change the world by\",\n",
    "    \"The country Malta is\"\n",
    "]\n",
    "\n",
    "print(\"=== Batch Completion Test ===\")\n",
    "for p in prompts:\n",
    "    print(f\"\\nPrompt: {p}\")\n",
    "    print(generate(p))\n",
    "    print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8923d9ed-34b0-4a37-ab6a-0b207c856dc4",
   "metadata": {},
   "source": [
    "## Generation Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad3c842-5841-443d-8db4-96ca4561430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[temperature = 0.7]\n",
      "A wise man once said the first to be used by the old in his life to his own own book. The \"Babaptized on the time\" of the same period, \"The life in the world has been an alternative by the city and the earliest, which is the only one of the other of the \"the world. In the first century, a period, the word of the first syllor and the name of the names of the \"the \"the names\" and \"a\n",
      "\n",
      "[temperature = 0.9]\n",
      "A wise man once said about the death of Christ, the king, they had no, to marry as the bishops and the new authority. The last other scholars were the church of the same time of the first two-day Church. As-day Christianity and religious leaders. It was also been a bishop and the church in the bishops to the church as to the church of England to the time of the church by the church of the Second Orthodox Church's political rights of the church of the Book of\n",
      "\n",
      "[temperature = 1.1]\n",
      "A wise man once said there.\" A few of a few different times that the two centuries of a \"The father's father's own most famous\", a number in the last books is the third century, and a similar sense of its only only by the Catholic Christianity, and the church which were the church, or, the early as a period was born on the Book of the other church, but in the Catholic of bishops believe that they would be a new church and Christianity. The Church was based\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A wise man once said\"\n",
    "for t in [0.7, 0.9, 1.1]:\n",
    "    print(f\"\\n[temperature = {t}]\")\n",
    "    print(generate(prompt, temperature=t, top_p=0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6457f-4bd9-40a1-a5ec-9202405cb49f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
